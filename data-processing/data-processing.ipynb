{"cells":[{"cell_type":"code","source":["from google.colab import drive\n","drive.mount('/content/drive')"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"kKkzQCOg8nra","executionInfo":{"status":"ok","timestamp":1680286211208,"user_tz":-300,"elapsed":63636,"user":{"displayName":"Hasan","userId":"07028982032628430753"}},"outputId":"e28f1cd0-5aa2-4f2f-a69d-c9a0c8b44eb3"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["Mounted at /content/drive\n"]}]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":12,"status":"ok","timestamp":1680286211211,"user":{"displayName":"Hasan","userId":"07028982032628430753"},"user_tz":-300},"id":"mZzUYKlAHXkc","outputId":"7e2bcd76-781f-42b6-b7e4-e74fb0a045f5"},"outputs":[{"output_type":"stream","name":"stdout","text":["/content/drive/MyDrive\n","/content/drive/MyDrive/Colab Notebooks\n","/content/drive/MyDrive/Colab Notebooks/nlp\n"]}],"source":["%cd drive/MyDrive\n","%cd 'Colab Notebooks'\n","%cd 'nlp'"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"jdHZZoUuHMX0"},"outputs":[],"source":["!pip install stanza\n","import stanza\n","# Import client module\n","from stanza.server import CoreNLPClient"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"NB9ezL0cJoKf"},"outputs":[],"source":["import pandas as pd\n","from collections import defaultdict\n","from heapq import nlargest\n","import string\n","import nltk\n","from nltk.tree import ParentedTree\n","import json\n","import pickle\n","from sklearn.feature_extraction.text import CountVectorizer\n","import numpy as np\n","from datetime import datetime as dt\n","import re"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":18,"status":"ok","timestamp":1678984297902,"user":{"displayName":"Hasan","userId":"07028982032628430753"},"user_tz":0},"id":"GVK7Q26nVW9H","outputId":"2463f006-770e-422b-f61a-1100c62fa2a3"},"outputs":[{"output_type":"stream","name":"stderr","text":["[nltk_data] Downloading package stopwords to /root/nltk_data...\n","[nltk_data]   Unzipping corpora/stopwords.zip.\n"]},{"output_type":"execute_result","data":{"text/plain":["True"]},"metadata":{},"execution_count":3}],"source":["nltk.download('stopwords')"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"Q3fvFD2nuiGB"},"outputs":[],"source":["df = pd.read_csv(\"cleaned-dataset.tsv\", sep='\\t')"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"F2tbaY5Inece"},"outputs":[],"source":["# extract season fromm match data\n","def process_season(date):\n","  dt_obj = dt.strptime(date, \"%b-%y\")\n","  m, y = dt_obj.month, dt_obj.year-2000\n","  # games before june in a year are from season that started in prev. calendar year\n","  if m < 6:\n","    res = f\"{y-1}/{y}\"\n","  else:\n","    res = f\"{y}/{y+1}\"\n","  return res\n","\n","df[\"Season\"] = df[\"Date\"].apply(process_season)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"IymaWLevJ_y3"},"outputs":[],"source":["all_teams = set(df[\"HomeTeam\"].unique()).union(set(df[\"AwayTeam\"].unique()))"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"TbYiOBJ75wTn"},"outputs":[],"source":["df.to_csv(\"cleaned-dataset.tsv\", sep='\\t')"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"R4vOtUbqUuAj"},"outputs":[],"source":["# pre-processing\n","def process_text(text):\n","  stopwords = nltk.corpus.stopwords.words('english')\n","  text = text.lower()\n","  text = \"\".join([i for i in text if i not in string.punctuation])\n","  text = text.split(' ')\n","  text = [i for i in text if i not in stopwords]\n","  return text"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"tzR7pVN6G2ZZ"},"outputs":[],"source":["# datasets containing player names for each season\n","# used to build key-term dictionaries\n","\n","def fifa_dataset_path(i):\n","  return f\"fifa-dataset/Fifa{i}-Players.csv\"\n","\n","player_df_13 = pd.read_csv(fifa_dataset_path(13))\n","player_df_14 = pd.read_csv(fifa_dataset_path(14))\n","player_df_15 = pd.read_csv(fifa_dataset_path(15))\n","player_df_16 = pd.read_csv(fifa_dataset_path(16))\n","player_df_17 = pd.read_csv(fifa_dataset_path(17))"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"ptlLx-6tH4t6"},"outputs":[],"source":["# make sure club names match\n","diff_names = set()\n","for player_df in [player_df_15, player_df_16, player_df_17]:\n","  \n","  team_translate = {\"West Ham\":\"West Ham United\", \"QPR\":\"Queens Park Rangers\", \"Newcastle Utd\":\"Newcastle United\",\n","                    \"Cardiff City\":\"Cardiff\", \"Spurs\":\"Tottenham Hotspur\", \"Manchester Utd\":\"Manchester United\"}\n","  player_df[\"Club\"] = player_df[\"Club\"].replace(team_translate)\n","  for team in all_teams: \n","    if not len(player_df[\"Name\"][player_df[\"Club\"] == team]):\n","      diff_names.add(team)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"cAm_zmzrsgZO"},"outputs":[],"source":["# make sure club names match\n","diff_names = set()\n","for player_df in [player_df_13, player_df_14]:\n","  \n","  team_translate = {\"West Bromwich Albion\":\"West Brom\", \"Huddersfield Town\":\"Huddersfield\",\n","                    \"Brighton &amp; Hove Albion\":\"Brighton\", \"Cardiff City\":\"Cardiff\"}\n","  player_df[\"Club\"] = player_df[\"Club\"].replace(team_translate)\n","  for team in all_teams: \n","    if not len(player_df[\"Name\"][player_df[\"Club\"] == team]):\n","      diff_names.add(team)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"H8CqH_6LtVV5"},"outputs":[],"source":["squads_13_14 = {}\n","for team in all_teams:\n","  squads_13_14[team] = list(player_df_13[\"Name\"][player_df_13[\"Club\"] == team])\n","\n","with open(\"squads_13_14.json\", \"w\") as f:\n","  json.dump(squads_13_14, f)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"6Q2pyWyCtWNI"},"outputs":[],"source":["squads_14_15 = {}\n","for team in all_teams:\n","  squads_14_15[team] = list(player_df_14[\"Name\"][player_df_14[\"Club\"] == team])\n","\n","with open(\"squads_14_15.json\", \"w\") as f:\n","  json.dump(squads_14_15, f)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"v7TVx77rJnKv"},"outputs":[],"source":["squads_15_16 = {}\n","for team in all_teams:\n","  squads_15_16[team] = list(player_df_15[\"Name\"][player_df_15[\"Club\"] == team])\n","\n","with open(\"squads_15_16.json\", \"w\") as f:\n","  json.dump(squads_15_16, f)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"1ltBzcwWL5Z6"},"outputs":[],"source":["squads_16_17 = {}\n","for team in all_teams:\n","  squads_16_17[team] = list(player_df_16[\"Name\"][player_df_16[\"Club\"] == team])\n","\n","with open(\"squads_16_17.json\", \"w\") as f:\n","  json.dump(squads_16_17, f)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"zxZSbwzvMoTG"},"outputs":[],"source":["squads_17_18 = {}\n","for team in all_teams:\n","  squads_17_18[team] = list(player_df_17[\"Name\"][player_df_17[\"Club\"] == team])\n","\n","with open(\"squads_17_18.json\", \"w\") as f:\n","  json.dump(squads_17_18, f)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"gHjLfzWzsZNN"},"outputs":[],"source":["with open(\"squads_13_14.json\", \"r\") as f:\n","  squads_13_14 = json.load(f)\n","\n","with open(\"squads_14_15.json\", \"r\") as f:\n","  squads_14_15 = json.load(f)\n","\n","with open(\"squads_15_16.json\", \"r\") as f:\n","  squads_15_16 = json.load(f)\n","\n","with open(\"squads_16_17.json\", \"r\") as f:\n","  squads_16_17 = json.load(f)\n","\n","with open(\"squads_17_18.json\", \"r\") as f:\n","  squads_17_18 = json.load(f)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"fB62aOQhosnV"},"outputs":[],"source":["!pip install stanza"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"N2I0Z-47rxjt"},"outputs":[],"source":["# reference https://stanfordnlp.github.io/CoreNLP/\n","# accessed 06/04/2023\n","# the Stanford CoreNLP client is used for OpenIE and parse tree extractions\n","import os\n","os.environ[\"CORENLP_HOME\"] = \"/content/drive/MyDrive/Colab Notebooks/nlp/stanford-corenlp-full-2018-10-05\""]},{"cell_type":"code","execution_count":null,"metadata":{"id":"jfrxa7nar6Ba"},"outputs":[],"source":["client = CoreNLPClient(timeout=150000000, be_quiet=True, annotators=['openie', 'parse', 'tokenize', 'pos', 'lemma', 'parse', 'depparse'], \n","endpoint='http://localhost:9002')\n","client.start()"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"dPFwOs0DJzqM"},"outputs":[],"source":["# reference, adapted from https://github.com/rahulkg31/sentence-to-clauses\n","# accessed 06/04/2023\n","# these functions print phrases from a parse tree generated by the CoreNLP client\n","\n","def get_verb_phrases(t):\n","    verb_phrases = []\n","    num_children = len(t)\n","    num_VP = sum(1 if t[i].label() == \"VP\" else 0 for i in range(0, num_children))\n","\n","    if t.label() != \"VP\":\n","        for i in range(0, num_children):\n","            if t[i].height() > 2:\n","                verb_phrases.extend(get_verb_phrases(t[i]))\n","    elif t.label() == \"VP\" and num_VP > 1:\n","        for i in range(0, num_children):\n","            if t[i].label() == \"VP\":\n","                if t[i].height() > 2:\n","                    verb_phrases.extend(get_verb_phrases(t[i]))\n","    else:\n","        verb_phrases.append(' '.join(t.leaves()))\n","\n","    return verb_phrases\n","\n","\n","def get_pos(t):\n","    vp_pos = []\n","    sub_conj_pos = []\n","    num_children = len(t)\n","    children = [t[i].label() for i in range(0, num_children)]\n","\n","    flag = re.search(r\"(S|SBAR|SBARQ|SINV|SQ)\", ' '.join(children))\n","\n","    if \"VP\" in children and not flag:\n","        for i in range(0, num_children):\n","            if t[i].label() == \"VP\":\n","                vp_pos.append(t[i].treeposition())\n","    elif not \"VP\" in children and not flag:\n","        for i in range(0, num_children):\n","            if t[i].height() > 2:\n","                temp1, temp2 = get_pos(t[i])\n","                vp_pos.extend(temp1)\n","                sub_conj_pos.extend(temp2)\n","    # comment this \"else\" part, if want to include subordinating conjunctions\n","    else:\n","        for i in range(0, num_children):\n","            if t[i].label() in [\"S\", \"SBAR\", \"SBARQ\", \"SINV\", \"SQ\"]:\n","                temp1, temp2 = get_pos(t[i])\n","                vp_pos.extend(temp1)\n","                sub_conj_pos.extend(temp2)\n","            else:\n","                sub_conj_pos.append(t[i].treeposition())\n","\n","    return (vp_pos, sub_conj_pos)\n","\n","\n","def print_clauses(parse_str):\n","    sent_tree = ParentedTree.fromstring(parse_str)\n","    clause_level_list = [\"S\", \"SBAR\", \"SBARQ\", \"SINV\", \"SQ\"]\n","    clause_list = []\n","    sub_trees = []\n","\n","    # break the tree into subtrees of clauses using\n","    # clause levels \"S\",\"SBAR\",\"SBARQ\",\"SINV\",\"SQ\"\n","    for sub_tree in reversed(list(sent_tree.subtrees())):\n","        if sub_tree.label() in clause_level_list:\n","            if sub_tree.parent().label() in clause_level_list:\n","                continue\n","\n","            if (len(sub_tree) == 1 and sub_tree.label() == \"S\" and sub_tree[0].label() == \"VP\"\n","                    and not sub_tree.parent().label() in clause_level_list):\n","                continue\n","\n","            sub_trees.append(sub_tree)\n","            del sent_tree[sub_tree.treeposition()]\n","\n","    # for each clause level subtree, extract relevant simple sentence\n","    for t in sub_trees:\n","        # get verb phrases from the new modified tree\n","        verb_phrases = get_verb_phrases(t)\n","\n","        # get tree without verb phrases (mainly subject)\n","        # remove subordinating conjunctions\n","        vp_pos, sub_conj_pos = get_pos(t)\n","        for i in reversed(vp_pos):\n","            del t[i]\n","        for i in reversed(sub_conj_pos):\n","            del t[i]\n","\n","        subject_phrase = ' '.join(t.leaves())\n","\n","        # update the clause_list\n","        for i in verb_phrases:\n","            clause_list.append(subject_phrase + \" \" + i)\n","\n","    print(clause_list)\n","    return clause_list"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"rA5ID8rwG6hz"},"outputs":[],"source":["# parse tree phrase extractions\n","\n","clauses_per_game = []\n","for text in df[\"Commentary\"]:\n","  document = client.annotate(text, output_format='json')\n","  curr_clauses = []\n","  parse_trees = []\n","  for sentence in document['sentences']:\n","      parse_tree = sentence['parse']\n","      parse_tree = ' '.join(parse_tree.split())\n","      parse_trees.append(parse_tree)\n","  \n","  for parse_tree in parse_trees:\n","    clause_list = print_clauses(parse_tree)\n","    for clause in clause_list:\n","      curr_clauses.append(clause)\n","\n","  clauses_per_game.append(curr_clauses)\n","\n","with open(\"clause-extractions\", \"wb\") as f:\n","  pickle.dump(clauses_per_game, f)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"gjduUjIJsMp6"},"outputs":[],"source":["# Stanford OpenIE extractions\n","\n","tuples_per_game = []\n","for text in df[\"Commentary\"]:\n","  document = client.annotate(text, output_format='json')\n","  triples = []\n","  for sentence in document['sentences']:\n","      for triple in sentence['openie']:\n","          triples.append(f\"{triple['subject']} {triple['relation']} {triple['object']}\")\n","    \n","  tuples_per_game.append(triples)"]},{"cell_type":"code","source":["# ClausIE extractions\n","# reference, adapted from https://github.com/mmxgn/spacy-clausie\n","# accessed 06/04/2023\n","# this module implements the ClausIE algorithm (https://resources.mpi-inf.mpg.de/d5/clausie/clausie-www13.pdf) in Python\n","\n","python -m pip install git+https://github.com/mmxgn/spacy-clausie.git\n","!python -m spacy download en_core_web_sm\n","\n","import spacy\n","import claucy\n","\n","clauses_per_game = []\n","nlp = spacy.load(\"en_core_web_sm\")\n","claucy.add_to_pipe(nlp)\n","\n","for text in df[\"Commentary\"]:\n","  doc = nlp(text)\n","  clause_list = []\n","  for clause in doc._.clauses:\n","    props = clause.to_propositions(inflect=None)\n","    for tup in props:\n","      clause_list.append(\" \".join([str(x) for x in tup]))\n","\n","  clauses_per_game.append(clause_list)\n","\n","with open(\"processed_data/spacy-clausie-extractions\", \"wb\") as f:\n","  pickle.dump(clauses_per_game, f)"],"metadata":{"id":"uuK2Fb2H0eTt"},"execution_count":null,"outputs":[]},{"cell_type":"code","execution_count":null,"metadata":{"id":"Vi1-5SGhmYsA"},"outputs":[],"source":["with open(\"processed_data/season_to_team_wordlist.json\", \"r\") as f:\n","  season_to_team_wordlist = json.load(f)\n","\n","with open(\"processed_data/extractions/clause-extractions\", \"rb\") as f:\n","  clauses_per_game = pickle.load(f)\n","\n","with open(\"processed_data/extractions/openie-extractions\", \"rb\") as f:\n","  tuples_per_game = pickle.load(f)\n","\n","with open(\"processed_data/extractions/spacy-clausie-extractions\", \"rb\") as f:\n","  spacy_clauses_per_game= pickle.load(f)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"CTLgfpxQLnHX"},"outputs":[],"source":["!pip install unidecode\n","import unidecode"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":2319,"status":"ok","timestamp":1679672863774,"user":{"displayName":"Hasan","userId":"07028982032628430753"},"user_tz":-300},"id":"mUSfJd0CJBXw","outputId":"c2868189-2b1b-41a5-f038-97787307b467"},"outputs":[{"output_type":"stream","name":"stderr","text":["[nltk_data] Downloading package punkt to /root/nltk_data...\n","[nltk_data]   Unzipping tokenizers/punkt.zip.\n"]}],"source":["# replace accented characters with non-accented\n","\n","from unidecode import unidecode\n","from nltk.tokenize import word_tokenize\n","nltk.download('punkt')\n","\n","unique_words = set()\n","for text in df[\"Commentary\"]:\n","  proc_text = word_tokenize(text)\n","  for word in proc_text:\n","    u = unidecode(word.lower(), \"utf-8\")\n","    unique_words.add(unidecode(u))"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"3em2LrSYPqWZ"},"outputs":[],"source":["word_to_idx = {w:i for i,w in enumerate(unique_words)}\n","with open(\"processed_data/word_to_idx\", \"wb\") as f:\n","  pickle.dump(word_to_idx, f)"]},{"cell_type":"code","source":["with open(\"processed_data/word_to_idx\", \"rb\") as f:\n","  word_to_idx = pickle.load(f)"],"metadata":{"id":"x6emAhWZRyRU"},"execution_count":null,"outputs":[]},{"cell_type":"code","execution_count":null,"metadata":{"id":"az92pyz9nMSA"},"outputs":[],"source":["def probability_from_wordlist(sentence, key_terms):\n","  # sentence should be a set of lower case words\n","  return sum([(w in key_terms) for w in sentence])/len(sentence)\n","\n","\n","def allocate_sentence_to_team(sentence, home, away, team_wordlist):\n","  home_prob = probability_from_wordlist(sentence, [x.lower() for x in team_wordlist[home]])\n","  away_prob = probability_from_wordlist(sentence, [x.lower() for x in team_wordlist[away]])\n","  res = None\n","  if home_prob > away_prob:\n","    res = home\n","  elif away_prob > home_prob:\n","    res = away\n","  \n","  return res"]},{"cell_type":"code","source":["# count vectorizer (bag of words)\n","cv = CountVectorizer(strip_accents='unicode', stop_words=nltk.corpus.stopwords.words('english'), \n","                              tokenizer = word_tokenize, vocabulary=word_to_idx)\n","\n","def process_sentences(home, away, sentence_list, team_wordlist, alpha=1, l=7576):\n","  home_vectors = []\n","  away_vectors = []\n","  for s in sentence_list:\n","    proc_text = word_tokenize(s)\n","    new_words = []\n","    for word in proc_text:\n","      u = unidecode(word.lower(), \"utf-8\")\n","      new_words.append(unidecode(u))\n","    \n","    allocation = allocate_sentence_to_team(new_words, home, away, team_wordlist)    \n","    vector = cv.fit_transform([\" \".join(new_words)]).toarray()\n","    if not l:\n","      l = vector.shape[1]\n","    vector = vector.reshape((1,l))\n","    \n","    if not allocation or allocation==home:\n","      home_vectors.append(vector)\n","    if not allocation or allocation==away:\n","      away_vectors.append(vector)\n","  \n","  res = np.zeros((2, l)) # res[0] = home team vector, res[1] = away team vector\n","\n","  # empty concatenate argument throws error so add zero vector\n","  home_vectors.append(np.zeros((1,l)))\n","  away_vectors.append(np.zeros((1,l)))\n","  res[0] = alpha*np.sum(np.concatenate(home_vectors), axis=0)\n","  res[1] = np.sum(np.concatenate(away_vectors), axis=0)\n","\n","  return res\n","\n","\n","def process_match(idx, df, all_tuples, season_to_team_wordlist):\n","  home, away, season = df[\"HomeTeam\"].iloc[idx], df[\"AwayTeam\"].iloc[idx], df[\"Season\"].iloc[idx]\n","  vectors = process_sentences(home, away, all_tuples[idx], season_to_team_wordlist[season])\n","  return vectors"],"metadata":{"id":"SNOerwHVSA_c"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# bag-of-words vectors for ClausIE\n","spacy_clausie_vectors = []\n","for i in range(len(df)):\n","  vectors = process_match(i, df, spacy_clauses_per_game, season_to_team_wordlist)\n","  spacy_clausie_vectors.append(vectors)\n","\n","new_clausie = [x.tolist() for x in spacy_clausie_vectors]\n","with open(\"processed_data/text_vectors/clausie_vectors_per_game\", \"wb\") as f:\n","  pickle.dump(new_clausie, f)"],"metadata":{"id":"mYg27Tpi4Jjx"},"execution_count":null,"outputs":[]},{"cell_type":"code","execution_count":null,"metadata":{"id":"G-SuwRosTsGo"},"outputs":[],"source":["# bag-of-words vectors for OpenIE\n","stanford_openie_vectors = []\n","for i in range(len(df)):\n","  vectors = process_match(i, df, tuples_per_game, season_to_team_wordlist)\n","  stanford_openie_vectors.append(vectors)\n","\n","new_openie = [x.tolist() for x in stanford_openie_vectors]\n","with open(\"processed_data/text_vectors/openie_vectors_per_game\", \"wb\") as f:\n","  pickle.dump(new_openie, f)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"QHo9zQ6NW66G"},"outputs":[],"source":["# bag-of-words vectors for parse tree clauses\n","stanford_tree_vectors = []\n","for i in range(len(df)):\n","  vectors = process_match(i, df, clauses_per_game, season_to_team_wordlist)\n","  stanford_tree_vectors.append(vectors)\n","\n","new_tree = [x.tolist() for x in stanford_tree_vectors]\n","with open(\"processed_data/text_vectors/parse_tree_vectors_per_game\", \"wb\") as f:\n","  pickle.dump(new_tree, f)"]},{"cell_type":"code","source":["# creating dataframe of extracted segment to team allocation\n","new_header = df.columns.values.tolist()+[\"Sentence\", \"Allocation\"]"],"metadata":{"id":"w0W7JMK8CEXH"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# OpenIE\n","\n","new_rows = []\n","for i, row in df.iterrows():\n","  sentence_list = tuples_per_game[i]\n","  home, away, season = row[\"HomeTeam\"], row[\"AwayTeam\"], row[\"Season\"]\n","  for s in sentence_list:\n","    proc_text = word_tokenize(s)\n","    new_words = []\n","    for word in proc_text:\n","      u = unidecode(word.lower(), \"utf-8\")\n","      new_words.append(unidecode(u))\n","    \n","    allocation = allocate_sentence_to_team(new_words, home, away, season_to_team_wordlist[season])   \n","    sentence = \" \".join(new_words)\n","    new_rows.append(list(row)+[sentence, allocation])\n","\n","pd.DataFrame(data=new_rows, columns=new_header).to_csv(\"openie_allocations.tsv\", sep='\\t')"],"metadata":{"id":"t4VXNr3Q_ju5"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# Parse tree\n","\n","new_rows = []\n","for i, row in df.iterrows():\n","  sentence_list = clauses_per_game[i]\n","  home, away, season = row[\"HomeTeam\"], row[\"AwayTeam\"], row[\"Season\"]\n","  for s in sentence_list:\n","    proc_text = word_tokenize(s)\n","    new_words = []\n","    for word in proc_text:\n","      u = unidecode(word.lower(), \"utf-8\")\n","      new_words.append(unidecode(u))\n","    \n","    allocation = allocate_sentence_to_team(new_words, home, away, season_to_team_wordlist[season])   \n","    sentence = \" \".join(new_words)\n","    new_rows.append(list(row)+[sentence, allocation])\n","\n","pd.DataFrame(data=new_rows, columns=new_header).to_csv(\"parse_tree_clause_allocations.tsv\", sep='\\t')"],"metadata":{"id":"OdP8S4S0Fo1P"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# ClausIE\n","\n","new_rows = []\n","for i, row in df.iterrows():\n","  sentence_list = spacy_clauses_per_game[i]\n","  home, away, season = row[\"HomeTeam\"], row[\"AwayTeam\"], row[\"Season\"]\n","  for s in sentence_list:\n","    proc_text = word_tokenize(s)\n","    new_words = []\n","    for word in proc_text:\n","      u = unidecode(word.lower(), \"utf-8\")\n","      new_words.append(unidecode(u))\n","    \n","    allocation = allocate_sentence_to_team(new_words, home, away, season_to_team_wordlist[season])   \n","    sentence = \" \".join(new_words)\n","    new_rows.append(list(row)+[sentence, allocation])\n","\n","pd.DataFrame(data=new_rows, columns=new_header).to_csv(\"clausie_allocations.tsv\", sep='\\t')"],"metadata":{"id":"Uqmearky43O0"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# dataframe of aggregated parse tree extractions\n","\n","new_rows = []\n","for i, row in df.iterrows():\n","  sentence_list = clauses_per_game[i]\n","  home, away, season = row[\"HomeTeam\"], row[\"AwayTeam\"], row[\"Season\"]\n","  home_sentences = []\n","  away_sentences = []\n","  for s in sentence_list:\n","    proc_text = word_tokenize(s)\n","    new_words = []\n","    for word in proc_text:\n","      u = unidecode(word.lower(), \"utf-8\")\n","      new_words.append(unidecode(u))\n","    \n","    allocation = allocate_sentence_to_team(new_words, home, away, season_to_team_wordlist[season])   \n","    sentence = \" \".join(new_words)\n","    if allocation == home:\n","      home_sentences.append(sentence)\n","    if allocation == away:\n","      away_sentences.append(sentence)\n","    if allocation is None:\n","      home_sentences.append(sentence)\n","      away_sentences.append(sentence)\n","\n","  new_rows.append(list(row)+[\" \".join(home_sentences), home])\n","  new_rows.append(list(row)+[\" \".join(away_sentences), away])\n","\n","pd.DataFrame(data=new_rows, columns=new_header).to_csv(\"parse_tree_clause_allocations_aggregated.tsv\", sep='\\t')"],"metadata":{"id":"Q3JVcDzAP2ix"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# dataframe of aggregated clausie extractions\n","\n","clausie_allocations = pd.read_csv('processed_data/extractions/clausie_allocations.tsv', sep='\\t').drop([\"Unnamed: 0\", \"Unnamed: 0.1\"], axis=1)\n","clausie_allocations = clausie_allocations.set_index([\"Season\", \"HomeTeam\", \"AwayTeam\"])\n","games = clausie_allocations.index.unique()\n","\n","final = []\n","for game in games:\n","  season, home, away = game\n","  home_sentences = []\n","  away_sentences = []\n","  rows = clausie_allocations.loc[game]\n","  for i, row in rows.iterrows():\n","    if row[\"Allocation\"] == home:\n","      home_sentences.append(row[\"Sentence\"])\n","    elif row[\"Allocation\"] == away:\n","      away_sentences.append(row[\"Sentence\"])\n","    else:\n","      home_sentences.append(row[\"Sentence\"])\n","      away_sentences.append(row[\"Sentence\"])\n","\n","  final.append([season, home, away, ' '.join(home_sentences), home])\n","  final.append([season, home, away, ' '.join(away_sentences), away])\n","\n","df = pd.DataFrame(final, columns=[\"Season\", \"HomeTeam\", \"AwayTeam\", \"Sentence\", \"Allocation\"])\n","df.to_csv('processed_data/extractions/clausie_allocations_aggregated.tsv', sep='\\t')"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"qiAzdwKz43wq","executionInfo":{"status":"ok","timestamp":1680287463304,"user_tz":-300,"elapsed":2270,"user":{"displayName":"Hasan","userId":"07028982032628430753"}},"outputId":"8d71b3c3-9eb7-497c-cf0e-0fc77438b739"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stderr","text":["<ipython-input-29-e52b5903472b>:6: PerformanceWarning: indexing past lexsort depth may impact performance.\n","  rows = clausie_allocations.loc[game]\n"]}]},{"cell_type":"code","source":["# dataframe of aggreated openie extractions\n","\n","openie_allocations = pd.read_csv('processed_data/extractions/openie_allocations.tsv', sep='\\t').drop([\"Unnamed: 0\", \"Unnamed: 0.1\"], axis=1)\n","openie_allocations = openie_allocations.set_index([\"Season\", \"HomeTeam\", \"AwayTeam\"])\n","games = openie_allocations.index.unique()\n","final = []\n","for game in games:\n","  season, home, away = game\n","  home_sentences = []\n","  away_sentences = []\n","  rows = openie_allocations.loc[game]\n","  for i, row in rows.iterrows():\n","    if row[\"Allocation\"] == home:\n","      home_sentences.append(row[\"Sentence\"])\n","    elif row[\"Allocation\"] == away:\n","      away_sentences.append(row[\"Sentence\"])\n","    else:\n","      home_sentences.append(row[\"Sentence\"])\n","      away_sentences.append(row[\"Sentence\"])\n","\n","  final.append([season, home, away, ' '.join(home_sentences), home])\n","  final.append([season, home, away, ' '.join(away_sentences), away])\n","\n","df = pd.DataFrame(final, columns=[\"Season\", \"HomeTeam\", \"AwayTeam\", \"Sentence\", \"Allocation\"])\n","df.to_csv('processed_data/extractions/openie_allocations_aggregated.tsv', sep='\\t')"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"Yl20Qx-n8T3E","executionInfo":{"status":"ok","timestamp":1680287655160,"user_tz":-300,"elapsed":3609,"user":{"displayName":"Hasan","userId":"07028982032628430753"}},"outputId":"9376f3d6-b1ad-4fd0-930c-5331fcee30cb"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stderr","text":["<ipython-input-34-5a80c910aaf8>:9: PerformanceWarning: indexing past lexsort depth may impact performance.\n","  rows = openie_allocations.loc[game]\n"]}]}],"metadata":{"colab":{"provenance":[]},"kernelspec":{"display_name":"Python 3","name":"python3"},"language_info":{"name":"python"}},"nbformat":4,"nbformat_minor":0}